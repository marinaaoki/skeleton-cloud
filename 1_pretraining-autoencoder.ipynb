{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-training the Auto-Encoder\n",
    "\n",
    "In this notebook, we pre-train the auto-encoder for reconstruction. In their paper, Carr et al. define the motion encoder `E_M` and the privacy encoder `E_P`, which are implemented identically as described below.\n",
    "\n",
    "![Encoder Architecture](figures/encoder.png)\n",
    "\n",
    "Here, the input size 75 refers to the frame length `T` of each sequence of 3D coordinates in the dataset. The output of each encoder is a motion embedding and a privacy embedding respectively. The acronyms are defined using the Pytorch implementations:\n",
    "\n",
    "- C2D: Convolution 2D\n",
    "- LR: Leaky ReLU\n",
    "- Up: Upsample\n",
    "- MP: Max Pooling\n",
    "- RP2D: Reflection Pad 2D\n",
    "\n",
    "This stage includes 5 epochs of paired pre-training, followed by 20 epochs of unpaired pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Paired Pre-training\n",
    "\n",
    "In order to separate the embeddings, we first start by pre-training the auto-encoder in the paired setting for 5 epochs.\n",
    "\n",
    "#### 1.1.1 Paired Data Loading\n",
    "\n",
    "In the paired setting, the model is trained with \"carefully matched sets of skeleton motions\". We load the preprocessed paired samples of the NTU RGB+D dataset, where two distinct actors perform the same two actions under an identical camera view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "==========\n",
      "NTU 60 Dataset\n",
      "==========\n",
      "Number of samples: 30757\n",
      "Number of frames: 75\n",
      "Number of joints: 25\n"
     ]
    }
   ],
   "source": [
    "from sitc.data.ntu60 import NTU60\n",
    "\n",
    "num_epochs = 5\n",
    "ntu60_dataset = NTU60(num_frames=75)\n",
    "\n",
    "print(f\"==========\\nNTU 60 Dataset\\n==========\")\n",
    "print(f\"Number of samples: {len(ntu60_dataset)}\")\n",
    "print(f\"Number of frames: {ntu60_dataset.max_frames}\")\n",
    "print(f\"Number of joints: {ntu60_dataset.num_joints}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: S001C003P004R002A038\n",
      "Camera ID: 3\n",
      "Action ID: 38\n",
      "Person ID: 4\n"
     ]
    }
   ],
   "source": [
    "sample = ntu60_dataset[0]\n",
    "print(f\"Sample ID: {sample['name']}\")\n",
    "print(f\"Camera ID: {sample['camera']}\")\n",
    "print(f\"Action ID: {sample['action']}\")\n",
    "print(f\"Person ID: {sample['person']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 25, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['keypoints'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(ntu60_dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "print(f\"Number of batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Instantiating the Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (motion_encoder): EM(\n",
       "    (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv3): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv4): Conv2d(32, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (lr): LeakyReLU(negative_slope=0.01)\n",
       "    (mp): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (rp): ReflectionPad2d((1, 1, 1, 1))\n",
       "  )\n",
       "  (privacy_encoder): PM(\n",
       "    (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv3): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv4): Conv2d(32, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (lr): LeakyReLU(negative_slope=0.01)\n",
       "    (mp): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (rp): ReflectionPad2d((1, 1, 1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (convt1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convt2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convt3): ConvTranspose2d(128, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convt4): ConvTranspose2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lr): LeakyReLU(negative_slope=0.01)\n",
       "    (up): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (rp): ReflectionPad2d((1, 1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sitc.models.autoencoder import AE\n",
    "\n",
    "ae = AE(in_features=3, out_features=256)\n",
    "ae.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Defining hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the reconstruction loss**\n",
    "\n",
    "The loss is defined as\n",
    "\n",
    "$$L_{rec} = \\mathbb{E}_{\\mathbf{s}\\sim\\mathcal{S}}\\Bigl[||\\mathit{D}(\\mathit{E_M}(\\mathbf{s}), \\mathit{E_P}(\\mathbf{s})) - \\mathbf{s} ||^2\\Bigr]$$\n",
    "\n",
    "This is equivalent to the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_rec = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the smooth loss**\n",
    "\n",
    "The smooth loss is defined as\n",
    "\n",
    "$$L_{smooth} = \\mathbb{E}_{\\mathbf{s}\\sim\\mathcal{S}}\\Biggl[\\frac{\\sqrt{\\sum_i^J|\\sum_j^T(\\hat{s}_{i,j}-\\hat{s}_{i,j+1})^2 - \\sum_j^T(\\mathbf{s}_{i,j}-\\mathbf{s}_{i,j+1}^2)}}{J\\times T}\\Biggr]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SmoothLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmoothLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        J = inputs.shape[0]\n",
    "        T = inputs.shape[1]\n",
    "        \n",
    "        total_shifts = []\n",
    "        for i in range(J): \n",
    "            input_shifts = []\n",
    "            target_shifts = []\n",
    "            for j in range(T):\n",
    "                input_shifts.append((inputs[i][j] - inputs[i][j+1]**2))\n",
    "                target_shifts.append((targets[i][j] - targets[i][j+1]**2))\n",
    "            \n",
    "            total_shifts.append(np.abs(sum(input_shifts) - sum(target_shifts)))\n",
    "        \n",
    "        loss = np.sqrt(sum(total_shifts)) / (J * T)\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "loss_smooth = SmoothLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the total loss**\n",
    "\n",
    "The total loss for the autoencoder pre-training is\n",
    "\n",
    "$$L_{ae} = \\alpha_{rec}L_{rec} + \\alpha_{smooth}L_{smooth}$$\n",
    "\n",
    "where $\\alpha_{rec}$, $\\alpha_{smooth}$ are hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AELoss(nn.Module):\n",
    "    def __init__(self, alpha_rec=2, alpha_smooth=3):\n",
    "        super(AELoss, self).__init__()\n",
    "        self.alpha_rec = alpha_rec\n",
    "        self.alpha_smooth = alpha_smooth\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        loss_rec = nn.MSELoss()\n",
    "        loss_smooth = SmoothLoss()\n",
    "\n",
    "        return self.alpha_rec * loss_rec(inputs, targets) + self.alpha_smooth * loss_smooth(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the optimiser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (motion_encoder): EM(\n",
       "    (conv1): Conv2d(75, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv3): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv4): Conv2d(32, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (lr): LeakyReLU(negative_slope=0.01)\n",
       "    (mp): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (rp): ReflectionPad2d((1, 1, 1, 1))\n",
       "  )\n",
       "  (privacy_encoder): PM(\n",
       "    (conv1): Conv2d(75, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv3): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv4): Conv2d(32, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (lr): LeakyReLU(negative_slope=0.01)\n",
       "    (mp): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (rp): ReflectionPad2d((1, 1, 1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (convt1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convt2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convt3): ConvTranspose2d(128, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (convt4): ConvTranspose2d(96, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lr): LeakyReLU(negative_slope=0.01)\n",
       "    (up): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (rp): ReflectionPad2d((1, 1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "optimiser = torch.optim.Adam(ae.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = AELoss()\n",
    "loss_fn.to('cuda')\n",
    "\n",
    "ae.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 75, 25, 3])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.71 GiB. GPU 0 has a total capacity of 9.77 GiB of which 496.75 MiB is free. Process 27918 has 249.38 MiB memory in use. Including non-PyTorch memory, this process has 8.69 GiB memory in use. Of the allocated memory 8.27 GiB is allocated by PyTorch, and 177.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/sitc/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/sitc/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/skeleton-cloud/sitc/models/autoencoder.py:22\u001b[0m, in \u001b[0;36mAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m privacy_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivacy_encoder(x)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((motion_embedding, privacy_embedding), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/sitc/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/sitc/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/skeleton-cloud/sitc/models/autoencoder.py:83\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvt1(x))))\n\u001b[1;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvt2(x))))\n\u001b[0;32m---> 83\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvt3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvt4(x))))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/sitc/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/sitc/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/sitc/lib/python3.9/site-packages/torch/nn/modules/upsampling.py:158\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubi-lab-desktop/Extreme Pro/sitc/lib/python3.9/site-packages/torch/nn/functional.py:4050\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest1d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n\u001b[1;32m   4049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_nearest2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest3d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.71 GiB. GPU 0 has a total capacity of 9.77 GiB of which 496.75 MiB is free. Process 27918 has 249.38 MiB memory in use. Including non-PyTorch memory, this process has 8.69 GiB memory in use. Of the allocated memory 8.27 GiB is allocated by PyTorch, and 177.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if i > 0:\n",
    "            break\n",
    "\n",
    "        batch = NTU60.move_batch_to_device(batch, 'cuda')\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        inputs = batch['keypoints'].float()\n",
    "\n",
    "        outputs = ae(inputs)\n",
    "\n",
    "        #loss = loss_fn(outputs, inputs)\n",
    "\n",
    "        #loss.backward()\n",
    "        #optimiser.step()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
